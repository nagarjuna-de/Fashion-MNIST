{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize torch vector and sample prediction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fashion(\n",
       "  (fc1): Linear(in_features=784, out_features=600, bias=True)\n",
       "  (bn1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (bn2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=400, out_features=100, bias=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (out): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (do): Dropout(p=0.2, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Fashion(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super(Fashion, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 600)\n",
    "        self.bn1 = nn.BatchNorm1d(600) # Batcn Norm for the first layer\n",
    "        self.fc2 = nn.Linear(600, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400) # Batcn Norm for the secon layer\n",
    "        self.fc3 = nn.Linear(400, 100)\n",
    "        self.bn3 = nn.BatchNorm1d(100)  # Batcn Norm for the third layer\n",
    "        self.out = nn.Linear(100, 10)\n",
    "        self.do  = nn.Dropout(0.2, inplace=True)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.do(self.fc1(x))))\n",
    "        x = F.relu(self.bn2(self.do(self.fc2(x))))\n",
    "        x = F.relu(self.bn3(self.do(self.fc3(x))))\n",
    "        return self.out(x)\n",
    "\n",
    "model = Fashion()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIteration: 0\t Loss: 0.0053\n",
      "\tIteration: 40\t Loss: 0.2797\n",
      "\tIteration: 80\t Loss: 0.2917\n",
      "\tIteration: 120\t Loss: 0.2884\n",
      "\tIteration: 160\t Loss: 0.2768\n",
      "\tIteration: 200\t Loss: 0.2756\n",
      "\tIteration: 240\t Loss: 0.2767\n",
      "\tIteration: 280\t Loss: 0.3058\n",
      "\tIteration: 320\t Loss: 0.3381\n",
      "\tIteration: 360\t Loss: 0.3114\n",
      "\tIteration: 400\t Loss: 0.3159\n",
      "\tIteration: 440\t Loss: 0.3038\n",
      "\tIteration: 480\t Loss: 0.3073\n",
      "\tIteration: 520\t Loss: 0.3064\n",
      "\tIteration: 560\t Loss: 0.2881\n",
      "\tIteration: 600\t Loss: 0.2980\n",
      "\tIteration: 640\t Loss: 0.3078\n",
      "\tIteration: 680\t Loss: 0.2735\n",
      "\tIteration: 720\t Loss: 0.2970\n",
      "\tIteration: 760\t Loss: 0.2809\n",
      "\tIteration: 800\t Loss: 0.3054\n",
      "\tIteration: 840\t Loss: 0.3098\n",
      "\tIteration: 880\t Loss: 0.3141\n",
      "\tIteration: 920\t Loss: 0.2920\n",
      "\tIteration: 0\t Loss: 0.0082\n",
      "\tIteration: 40\t Loss: 0.2761\n",
      "\tIteration: 80\t Loss: 0.2639\n",
      "\tIteration: 120\t Loss: 0.2785\n",
      "\tIteration: 160\t Loss: 0.2815\n",
      "\tIteration: 200\t Loss: 0.3080\n",
      "\tIteration: 240\t Loss: 0.2843\n",
      "\tIteration: 280\t Loss: 0.2798\n",
      "\tIteration: 320\t Loss: 0.2755\n",
      "\tIteration: 360\t Loss: 0.3108\n",
      "\tIteration: 400\t Loss: 0.2635\n",
      "\tIteration: 440\t Loss: 0.2820\n",
      "\tIteration: 480\t Loss: 0.2883\n",
      "\tIteration: 520\t Loss: 0.2792\n",
      "\tIteration: 560\t Loss: 0.2863\n",
      "\tIteration: 600\t Loss: 0.3129\n",
      "\tIteration: 640\t Loss: 0.3012\n",
      "\tIteration: 680\t Loss: 0.2618\n",
      "\tIteration: 720\t Loss: 0.2733\n",
      "\tIteration: 760\t Loss: 0.2922\n",
      "\tIteration: 800\t Loss: 0.3024\n",
      "\tIteration: 840\t Loss: 0.2558\n",
      "\tIteration: 880\t Loss: 0.2897\n",
      "\tIteration: 920\t Loss: 0.3095\n",
      "\tIteration: 0\t Loss: 0.0053\n",
      "\tIteration: 40\t Loss: 0.2660\n",
      "\tIteration: 80\t Loss: 0.2528\n",
      "\tIteration: 120\t Loss: 0.2506\n",
      "\tIteration: 160\t Loss: 0.2839\n",
      "\tIteration: 200\t Loss: 0.3006\n",
      "\tIteration: 240\t Loss: 0.2944\n",
      "\tIteration: 280\t Loss: 0.2625\n",
      "\tIteration: 320\t Loss: 0.2688\n",
      "\tIteration: 360\t Loss: 0.2561\n",
      "\tIteration: 400\t Loss: 0.2630\n",
      "\tIteration: 440\t Loss: 0.2559\n",
      "\tIteration: 480\t Loss: 0.2659\n",
      "\tIteration: 520\t Loss: 0.2651\n",
      "\tIteration: 560\t Loss: 0.2781\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rujek\\Documents\\GitHub\\Deep_learning\\Fashion-MNIST\\fashion.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rujek/Documents/GitHub/Deep_learning/Fashion-MNIST/fashion.ipynb#ch0000036?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rujek/Documents/GitHub/Deep_learning/Fashion-MNIST/fashion.ipynb#ch0000036?line=7'>8</a>\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rujek/Documents/GitHub/Deep_learning/Fashion-MNIST/fashion.ipynb#ch0000036?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39miter\u001b[39m(trainloader)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rujek/Documents/GitHub/Deep_learning/Fashion-MNIST/fashion.ipynb#ch0000036?line=9'>10</a>\u001b[0m         images\u001b[39m.\u001b[39mresize_(images\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m784\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rujek/Documents/GitHub/Deep_learning/Fashion-MNIST/fashion.ipynb#ch0000036?line=11'>12</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()   \u001b[39m# 1st step: reset the gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\strive\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\strive\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\strive\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\strive\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\strive\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/datasets/mnist.py?line=141'>142</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/datasets/mnist.py?line=143'>144</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/datasets/mnist.py?line=144'>145</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/datasets/mnist.py?line=146'>147</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/datasets/mnist.py?line=147'>148</a>\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\strive\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=92'>93</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=93'>94</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=94'>95</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\strive\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=126'>127</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=127'>128</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=128'>129</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=129'>130</a>\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=132'>133</a>\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=133'>134</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/transforms.py?line=134'>135</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\strive\\lib\\site-packages\\torchvision\\transforms\\functional.py:151\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/functional.py?line=148'>149</a>\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/functional.py?line=149'>150</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/functional.py?line=150'>151</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/functional.py?line=151'>152</a>\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/rujek/Anaconda3/envs/strive/lib/site-packages/torchvision/transforms/functional.py?line=152'>153</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "print_every = 40\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "#accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "        images.resize_(images.size()[0], 784)\n",
    "    \n",
    "        optimizer.zero_grad()   # 1st step: reset the gradients\n",
    "\n",
    "        logits = model.forward(images) #2nd step: make the prediction\n",
    "        output = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        train_loss = criterion(output, labels)   #3rd step: compute the loss\n",
    "\n",
    "        train_loss.backward() #4th step: backward pass\n",
    "\n",
    "        optimizer.step() #5th step: save the weights\n",
    "        running_loss += train_loss.item()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(iter(testloader)):\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            logits = model.forward(images)\n",
    "            output = F.log_softmax(logits, dim=1)\n",
    "\n",
    "            test_loss = criterion(output, labels)\n",
    "\n",
    "                #classes = output > 0.5\n",
    "\n",
    "                #acc = sum(classes == labels) / classes.shape[0]\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    #accuracies.append(acc)\n",
    "    print(f'Epoch: {epoch + 1} | loss: {train_loss.item()} | test loss: {test_loss.item()}') #| accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train Loss')\n",
    "plt.plot(test_losses, label='test Loss')\n",
    "#plt.plot(accuracies, label='accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check predicts well using view classify\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "logit = model.forward(images[0,:])\n",
    "ps = F.softmax(logit, dim =1)\n",
    "print(ps)\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tips for validation\n",
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Look at the most likely classes for the first 10 examples\n",
    "print(top_class[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals = top_class == labels.view(*top_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = [index for index,value in enumerate(equals) if value.item() is False] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d9dac11d33c8124df76be2d536cf81855f1e36a1c8ea739aa2eea81492656c9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('strive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
